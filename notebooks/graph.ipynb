{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lung cancer classification with Graph Convolutional Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"README.md\"):\n",
    "    os.chdir(\"../\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for the ML part\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "# for the graph part\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import to_networkx, from_networkx\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from scripts.gcn import GCN, train, test, train_loop\n",
    "BATCH_SIZE = 16\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import dataset from csv files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degenes = pd.read_csv('./data/final/degenes.csv', index_col=0)\n",
    "pdata = pd.read_csv('./data/final/pdata_nan_filled.csv', index_col=0)\n",
    "\n",
    "degenes_t = degenes.T\n",
    "degenes_t.columns = [x.split('///')[0] for x in degenes_t.columns]\n",
    "degenes = degenes_t.T\n",
    "degenes = degenes/10\n",
    "degenes_t = degenes.T/10\n",
    "\n",
    "matrix = pd.read_csv('data/final/adj_matrix.csv', index_col=0)\n",
    "matrix = matrix.drop('cancer_status', axis=1).drop('cancer_status', axis=0)\n",
    "\n",
    "degenes_t.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop nan in pdata\n",
    "pdata = pdata.dropna(axis=0)\n",
    "pdata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMaxScaler and StandardScaler pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min max scale\n",
    "degenes_scaled = pd.DataFrame(degenes, index=degenes.index, columns=degenes.columns)\n",
    "#degenes_scaled = degenes_scaled.applymap(lambda x: np.exp(x))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "degenes_scaled = pd.DataFrame(scaler.fit_transform(degenes_scaled), index=degenes_scaled.index, columns=degenes_scaled.columns)\n",
    "\n",
    "mmscaler = MinMaxScaler()\n",
    "degenes_scaled = pd.DataFrame(mmscaler.fit_transform(degenes_scaled), index=degenes_scaled.index, columns=degenes_scaled.columns)\n",
    "\n",
    "# minmax pdata\n",
    "pdata_scaled = pd.DataFrame(pdata, index=pdata.index, columns=pdata.columns)\n",
    "pdata_scaled = pd.DataFrame(scaler.fit_transform(pdata_scaled), index=pdata_scaled.index, columns=pdata_scaled.columns)\n",
    "pdata_scaled = pd.DataFrame(mmscaler.fit_transform(pdata_scaled), index=pdata_scaled.index, columns=pdata_scaled.columns)\n",
    "\n",
    "degenes_scaled.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdata_scaled.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building graph structure from adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = {}\n",
    "\n",
    "for i in range(0, len(degenes_scaled.columns)):\n",
    "    G = nx.from_pandas_adjacency(matrix)\n",
    "    G.remove_nodes_from(list(nx.isolates(G)))\n",
    "    nx.set_node_attributes(G, degenes_scaled.iloc[:,i].to_dict(), 'x')\n",
    "\n",
    "    for edge in G.edges:\n",
    "        G.edges[edge]['weight'] = 1\n",
    "\n",
    "    graphs[degenes_scaled.columns[i]] = G"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pytorch graph structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a geometric data object from the networkx for each graph\n",
    "data_list = []\n",
    "for key, value in graphs.items():\n",
    "    try:\n",
    "        cs = pdata.loc[key, 'cancer_status']\n",
    "\n",
    "        d = from_networkx(value)\n",
    "        d.x = torch.tensor([d[1]['x'] for d in value.nodes(data=True)], dtype=torch.float32)\n",
    "        d.x = d.x.view(-1, 1)\n",
    "\n",
    "        target = torch.tensor([[0, 1]], dtype=torch.float32) if cs == 1 else torch.tensor([[1, 0]], dtype=torch.float32)\n",
    "        additional_features = pdata.loc[key].drop(['cancer_status','subjective_assessment'], axis=0)\n",
    "        additional_features = additional_features.to_frame().T\n",
    "        additional_features = additional_features.astype('float32')\n",
    "\n",
    "        d.y = [target, torch.tensor(additional_features.values)]\n",
    "\n",
    "        data_list.append(d)\n",
    "    except:\n",
    "        KeyError"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in train validation and test\n",
    "train_data, test_data = train_test_split(data_list, test_size=0.3, random_state=42)\n",
    "val_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)\n",
    "\n",
    "len(train_data), len(test_data), len(val_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create torch DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "train_cancer_status = [torch.argmax(d.y[0]).item() for d in train_data]\n",
    "val_cancer_status = [torch.argmax(d.y[0]).item() for d in val_data]\n",
    "test_cancer_status = [torch.argmax(d.y[0]).item() for d in test_data]\n",
    "\n",
    "print('train_data: ', Counter(train_cancer_status))\n",
    "print('val_data: ', Counter(val_cancer_status))\n",
    "print('test_data: ', Counter(test_cancer_status))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, graph classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from scripts.gcn import GCN, EarlyStopping, train_loop, test, GCNClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(hidden_channels=256)\n",
    "print(model)\n",
    "\n",
    "# define training loop\n",
    "device = torch.device(\"cpu\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-3)\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop(\n",
    "    model, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    epochs=1000,\n",
    "    early_stopping=EarlyStopping(patience=50, delta=0.0001, verbose=True),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "128 hidden units, 0.2 dropout, 1e-3 L2, 0.001 learning rate\n",
    "Best train loss: 0.1018\tBest val loss: 0.1436"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, loss = test(test_loader, model, criterion)\n",
    "print(f'Test Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_set = train_data.copy()\n",
    "for x in val_data:\n",
    "    dev_set.append(x)\n",
    "\n",
    "dev_loader = DataLoader(dev_set, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(hidden_channels=64)\n",
    "print(model)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# define training loop\n",
    "device = torch.device(\"cpu\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-3)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "train_loop(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    dev_loader,\n",
    "    dev_loader,\n",
    "    epochs=1000,\n",
    "    verbose=True,\n",
    "    min_loss = 0.08\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on test set\n",
    "acc, loss = test(test_loader, model, criterion)\n",
    "print(f'Test Accuracy: {acc:.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# autoreload \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from scripts.gcn import GCN, EarlyStopping, train_loop, test, GCNClassifier\n",
    "\n",
    "# lucky config K=3, rs=43, 256 hidden, 0.01 lr, 1e-3 wd\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=43)\n",
    "accuracies =[]\n",
    "losses = []\n",
    "counter = 1\n",
    "torch.manual_seed(42)\n",
    "for train_index, test_index in kf.split(data_list):\n",
    "\n",
    "    train_data = [data_list[i] for i in train_index]\n",
    "    # append the noisy data to the train data\n",
    "\n",
    "    train_data_noisy = train_data.copy()\n",
    "    \n",
    "    \"\"\"\n",
    "    for t in train_data_noisy:\n",
    "        t.x = t.x + (0.5**0.5)*torch.randn(t.x.shape)\n",
    "\n",
    "    for t in train_data_noisy:\n",
    "        train_data.append(t)\n",
    "    \"\"\"\n",
    "\n",
    "    test_data = [data_list[i] for i in test_index]\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = GCN(hidden_channels=256)\n",
    "    print(model)\n",
    "\n",
    "    # define training loop\n",
    "    device = torch.device(\"cpu\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-3)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    train_loop(\n",
    "        model,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        train_loader,\n",
    "        test_loader,\n",
    "        epochs=1000,\n",
    "        verbose=False,\n",
    "        min_loss=0.1\n",
    "    )\n",
    "    model.eval()\n",
    "    acc, loss = test(test_loader, model, criterion)\n",
    "    accuracies.append(acc)\n",
    "    losses.append(loss)\n",
    "    print(f'> Fold {counter} trained. Test accuracy: {acc:.3f}\\tTest loss {loss:.3f}')\n",
    "    counter += 1\n",
    "\n",
    "# print mean accuracy and loss\n",
    "print('-'*20)\n",
    "print('REPORT')\n",
    "print(f'Mean accuracy: {np.mean(accuracies):.3f} ')\n",
    "print(f'Mean loss: {np.mean(losses):.3f} ')\n",
    "print('-'*20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
